
agora que integrar essa ui com pyspark, a ideia e iniciar uma sessao do spark, sendo antes de comecar a sessao, preciso setar o caminho do metastore_db e o warehouse_dir, eu aperto alguma atalho e abre um popup para colocar esses caminhos, e ao colocar esse caminho ele fica salvo para quando iniciar uma sessao de novo eles estarem configurados.


packages = [
            "io.delta:delta-spark_2.12:3.1.0",
        ]
        metastore_db = ""
        warehouse_dir = ""
        return (SparkSession.getActiveSession() or
                SparkSession.builder
                .appName("LocalDeltaPipeline")
                .master("local[*]")
                .config("spark.jars.packages", ",".join(packages))
                .config("spark.sql.extensions",
                        "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog",
                        "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .config("spark.sql.warehouse.dir", warehouse_dir)
                .config("spark.sql.catalogImplementation", "hive")
                .config('spark.hadoop.javax.jdo.option.ConnectionURL',f"jdbc:derby:;databaseName={metastore_db};create=true")     
                .enableHiveSupport()
                .getOrCreate())

quando comecar iniciar a sessao, mostrar algo visual informando que spark comecou a iniciar a sessao e quando terminar um feedback informando que spark foi inciado.


apois iniciar a sessao, no sidebar, carrega as bancos e as tabelas.

ao clicar no tabela ao lado, faz automaticamente um spark.sql("select * from tabela x"), e o textarea ficara para fazer querys, mas nao vou usar spark.sql, quero somente passar a query sql, e quando a query comecar a carregar, mostrar um loading na area da tabela informando que esta carregando, e a tabela precisa esta na mesma estrtura, tipo e nome da columa.


aproveita e monsta um script um spark, para criar umas 6 tabelas, com 500 linhas cada, apra testar o catalog.


